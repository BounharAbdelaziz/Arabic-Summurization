{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/abounhar/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/infres/abounhar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/infres/abounhar/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/infres/abounhar/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabdelazizbounhar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Training configuration:\n",
      "{'BASE_MODEL': 'Qwen2.5-0.5B-Instruct',\n",
      " 'DATASET_PATH': 'BounharAbdelaziz/Arabic-Synthetic-Summarization-Dataset-Filtered',\n",
      " 'FP16_TRAINING': True,\n",
      " 'MAX_TRAINING_SAMPLES': 1000,\n",
      " 'METRIC_FOR_BEST_MODEL': 'rougeL',\n",
      " 'MODELS_DICT': {'Falcon3-1B-Base': {'CAUSAL_LM': True,\n",
      "                                     'MODEL_PATH': 'tiiuae/Falcon3-1B-Base',\n",
      "                                     'SFT_TRAINING': False},\n",
      "                 'Falcon3-1B-Base-SFT': {'CAUSAL_LM': True,\n",
      "                                         'MODEL_PATH': 'tiiuae/Falcon3-1B-Base',\n",
      "                                         'SFT_TRAINING': True},\n",
      "                 'Falcon3-1B-Instruct': {'CAUSAL_LM': True,\n",
      "                                         'MODEL_PATH': 'tiiuae/Falcon3-1B-Instruct',\n",
      "                                         'SFT_TRAINING': True},\n",
      "                 'Falcon3-3B-Instruct': {'CAUSAL_LM': True,\n",
      "                                         'MODEL_PATH': 'tiiuae/Falcon3-3B-Instruct',\n",
      "                                         'SFT_TRAINING': True},\n",
      "                 'MINE': {'CAUSAL_LM': True,\n",
      "                          'MODEL_PATH': 'BounharAbdelaziz/Falcon3-1B-Instruct-bs-1-lr-2e-05-ep-3-wmp-100-gacc-32-gnorm-1.0-FP16-SFT-mxln-1024',\n",
      "                          'SFT_TRAINING': True},\n",
      "                 'Qwen2.5-0.5B': {'CAUSAL_LM': True,\n",
      "                                  'MODEL_PATH': 'Qwen/Qwen2.5-0.5B',\n",
      "                                  'SFT_TRAINING': False},\n",
      "                 'Qwen2.5-0.5B-Instruct': {'CAUSAL_LM': True,\n",
      "                                           'MODEL_PATH': 'Qwen/Qwen2.5-0.5B-Instruct',\n",
      "                                           'SFT_TRAINING': True},\n",
      "                 'Qwen2.5-0.5B-SFT': {'CAUSAL_LM': True,\n",
      "                                      'MODEL_PATH': 'Qwen/Qwen2.5-0.5B',\n",
      "                                      'SFT_TRAINING': True},\n",
      "                 'Qwen2.5-3B-Instruct': {'CAUSAL_LM': True,\n",
      "                                         'MODEL_PATH': 'Qwen/Qwen2.5-3B-Instruct',\n",
      "                                         'SFT_TRAINING': True},\n",
      "                 'bert-base-arabic': {'CAUSAL_LM': False,\n",
      "                                      'MODEL_PATH': 'asafaya/bert-base-arabic',\n",
      "                                      'SFT_TRAINING': False},\n",
      "                 'gpt2': {'CAUSAL_LM': True,\n",
      "                          'MODEL_PATH': 'openai-community/gpt2',\n",
      "                          'SFT_TRAINING': False},\n",
      "                 'mt5-base': {'CAUSAL_LM': False,\n",
      "                              'MODEL_PATH': 'google/mt5-base',\n",
      "                              'SFT_TRAINING': False},\n",
      "                 'mt5-base-SFT': {'CAUSAL_LM': False,\n",
      "                                  'MODEL_PATH': 'google/mt5-base',\n",
      "                                  'SFT_TRAINING': True},\n",
      "                 'mt5-small': {'CAUSAL_LM': False,\n",
      "                               'MODEL_PATH': 'google/mt5-small',\n",
      "                               'SFT_TRAINING': False},\n",
      "                 'mt5-small-SFT': {'CAUSAL_LM': False,\n",
      "                                   'MODEL_PATH': 'google/mt5-small',\n",
      "                                   'SFT_TRAINING': True}},\n",
      " 'SEED': 42,\n",
      " 'hyperparameters': {'MAX_LEN': 1024,\n",
      "                     'USE_LORA': False,\n",
      "                     'batch_size': 2,\n",
      "                     'eval_steps': 100,\n",
      "                     'gradient_accumulation_steps': 32,\n",
      "                     'logging_steps': 50,\n",
      "                     'lora_alpha': 32,\n",
      "                     'lora_dropout': 0.05,\n",
      "                     'lora_r': 128,\n",
      "                     'lr': 5e-06,\n",
      "                     'max_grad_norm': 1.0,\n",
      "                     'num_train_epochs': 1,\n",
      "                     'optimizer': 'adamw_torch_fused',\n",
      "                     'save_steps': 100,\n",
      "                     'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
      "                     'warmup_ratio': 0.04,\n",
      "                     'warmup_steps': 100}}\n",
      "--------------------------------------------------\n",
      "[INFO] Truncating training samples to: 1000...\n",
      "[INFO] Dataset loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary', 'summary_model_name', 'tokenizer_name', 'dataset_source', 'sequence_length'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'summary', 'summary_model_name', 'tokenizer_name', 'dataset_source', 'sequence_length'],\n",
      "        num_rows: 437\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'summary', 'summary_model_name', 'tokenizer_name', 'dataset_source', 'sequence_length'],\n",
      "        num_rows: 444\n",
      "    })\n",
      "})\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model and Tokenizer loaded: Qwen/Qwen2.5-0.5B-Instruct, version: Qwen2.5-0.5B-Instruct, IS_SFT_TRAINING: True, FP16_TRAINING: True\n",
      "--------------------------------------------------\n",
      "Configuration saved to ./run_configs/meh.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/infres/abounhar/mbzuai/train/wandb/run-20250127_122336-r1qqiwvk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdelazizbounhar/arabic-summarization-v2/runs/r1qqiwvk' target=\"_blank\">meh</a></strong> to <a href='https://wandb.ai/abdelazizbounhar/arabic-summarization-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdelazizbounhar/arabic-summarization-v2' target=\"_blank\">https://wandb.ai/abdelazizbounhar/arabic-summarization-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdelazizbounhar/arabic-summarization-v2/runs/r1qqiwvk' target=\"_blank\">https://wandb.ai/abdelazizbounhar/arabic-summarization-v2/runs/r1qqiwvk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/abdelazizbounhar/arabic-summarization-v2/runs/r1qqiwvk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x745408658b30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "# Import necessary libraries\n",
    "import wandb\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    # DataCollatorWithPadding,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "from utils import(\n",
    "    preprocess_function_seq2seq,\n",
    "    preprocess_function_causal_lm,\n",
    "    preprocess_function_causal_lm_sft_training,\n",
    "    preprocess_function_causal_lm_sft_testing,\n",
    "    preprocess_logits_for_metrics,\n",
    "    compute_metrics,\n",
    "    compute_metrics_causal_lm,\n",
    "    set_seed,\n",
    "    print_trainable_params_info,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "# Set up logging and tracking\n",
    "wandb.login()\n",
    "\n",
    "# get training configuration\n",
    "with open('training_config.yaml') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "print('-'*50)\n",
    "print(\"Training configuration:\")\n",
    "pprint(config)\n",
    "print('-'*50)\n",
    "\n",
    "MODELS_DICT = config['MODELS_DICT']\n",
    "\n",
    "# Training hyperparameters\n",
    "num_train_epochs = config['hyperparameters']['num_train_epochs']\n",
    "lr = config['hyperparameters']['lr']\n",
    "batch_size = config['hyperparameters']['batch_size']\n",
    "gradient_accumulation_steps = config['hyperparameters']['gradient_accumulation_steps']\n",
    "max_grad_norm = config['hyperparameters']['max_grad_norm']\n",
    "warmup_steps = config['hyperparameters']['warmup_steps']\n",
    "warmup_ratio = config['hyperparameters']['warmup_ratio']\n",
    "\n",
    "# Logging and saving\n",
    "logging_steps = config['hyperparameters']['logging_steps']\n",
    "save_steps = config['hyperparameters']['save_steps']\n",
    "eval_steps = config['hyperparameters']['eval_steps']\n",
    "\n",
    "# Training data path\n",
    "TRAIN_DATA_PATH = config['DATASET_PATH']\n",
    "\n",
    "# base model path\n",
    "BASE_MODEL = config['BASE_MODEL']\n",
    "MODEL_PATH = MODELS_DICT[BASE_MODEL]['MODEL_PATH']\n",
    "IS_CAUSAL_LM = MODELS_DICT[BASE_MODEL]['CAUSAL_LM']\n",
    "IS_SFT_TRAINING = MODELS_DICT[BASE_MODEL]['SFT_TRAINING']\n",
    "FP16_TRAINING = config['FP16_TRAINING']\n",
    "\n",
    "# max training samples\n",
    "MAX_TRAINING_SAMPLES = config['MAX_TRAINING_SAMPLES']\n",
    "\n",
    "if FP16_TRAINING:\n",
    "    torch_dtype=torch.bfloat16 # bfloat16 has better precission than float16 thanks to bigger mantissa. Though not available with all GPUs architecture.\n",
    "else:\n",
    "    torch_dtype=torch.float32\n",
    "\n",
    "# set seed\n",
    "SEED = config['SEED']\n",
    "set_seed(SEED)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(TRAIN_DATA_PATH)  # Replace with your dataset path\n",
    "\n",
    "# truncate training dataset to observe data size impact on performance\n",
    "print(f'[INFO] Truncating training samples to: {MAX_TRAINING_SAMPLES}...')\n",
    "dataset['train'] = dataset['train'].select(range(min(len(dataset['train']), MAX_TRAINING_SAMPLES)))\n",
    "print(f'[INFO] Dataset loaded: {dataset}')\n",
    "print('-'*50)\n",
    "\n",
    "# Load tokenizer and model\n",
    "if IS_CAUSAL_LM:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=torch_dtype,\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=torch_dtype, \n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "if config['hyperparameters']['USE_LORA']:\n",
    "    # Apply LoRA\n",
    "    print(f\"[INFO] Training with LoRA\")\n",
    "    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "    \n",
    "    # Define LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=config['hyperparameters']['lora_r'],\n",
    "        lora_alpha=config['hyperparameters']['lora_alpha'],\n",
    "        lora_dropout=config['hyperparameters']['lora_dropout'],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\" if IS_CAUSAL_LM else \"SEQ_2_SEQ_LM\",  # Adjust for your task\n",
    "        target_modules=config['hyperparameters']['target_modules'],  # Specify target modules if required\n",
    "    )\n",
    "    \n",
    "    # Wrap the model with LoRA\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Log trainable parameters for verification\n",
    "    print_trainable_params_info(model)\n",
    "\n",
    "    print('-'*50)\n",
    "\n",
    "# Set a maximum length for tokenization\n",
    "tokenizer.model_max_length = config['hyperparameters']['MAX_LEN']\n",
    "if BASE_MODEL == \"gpt2\":\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "print(f'[INFO] Model and Tokenizer loaded: {MODEL_PATH}, version: {BASE_MODEL}, IS_SFT_TRAINING: {IS_SFT_TRAINING}, FP16_TRAINING: {FP16_TRAINING}')\n",
    "print('-'*50)\n",
    "\n",
    "# Project name for loggings and savings\n",
    "project_name = \"arabic-summarization-v2\"\n",
    "fp16 = '-FP16' if FP16_TRAINING else ''\n",
    "sft = '-SFT' if IS_SFT_TRAINING else ''\n",
    "# LoRA params\n",
    "lora_training = f'-lora' if config['hyperparameters']['USE_LORA'] else ''\n",
    "lora_r = f'-r-{config['hyperparameters']['lora_r']}' if config['hyperparameters']['USE_LORA'] else ''\n",
    "lora_alpha = f'-a-{config['hyperparameters']['lora_alpha']}' if config['hyperparameters']['USE_LORA'] else ''\n",
    "lora_dropout = f'-d-{config['hyperparameters']['lora_dropout']}' if config['hyperparameters']['USE_LORA'] else ''\n",
    "\n",
    "run_name = 'meh' #f'{MODEL_PATH.split(\"/\")[-1]}-bs-{batch_size}-lr-{lr}-ep-{num_train_epochs}-wp-{warmup_steps}-gacc-{gradient_accumulation_steps}-gnm-{max_grad_norm}{fp16}{sft}-mx-{config['hyperparameters']['MAX_LEN']}{lora_training}{lora_r}{lora_alpha}-v2'\n",
    "assert '--' not in run_name, f\"[WARN] Detected -- in run_name. This will cause a push_to_hub error! Found run_name={run_name} \"\n",
    "assert len(run_name) < 96, f\"[WARN] run_name too long. This will cause a push_to_hub error! Consider squeezing it. Found run_name={run_name}\"\n",
    "\n",
    "# Where to save the model\n",
    "MODEL_RUN_SAVE_PATH = f\"BounharAbdelaziz/{run_name}\"\n",
    "\n",
    "# Save the configuration to a .txt file\n",
    "output_filename = f\"./run_configs/{run_name}.txt\"\n",
    "with open(output_filename, 'w') as output_file:\n",
    "    for key, value in config.items():\n",
    "        output_file.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "print(f\"Configuration saved to {output_filename}\")\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged, all runs will be under this project\n",
    "    project=project_name,   \n",
    "    # Group runs by model size\n",
    "    group=MODEL_PATH,       \n",
    "    # Unique run name\n",
    "    name=run_name,\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_train_epochs\": num_train_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"warmup_ratio\": warmup_ratio,\n",
    "        # \"warmup_steps\": warmup_steps,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "        # \"weight_decay\": weight_decay,\n",
    "        \"dataset\": TRAIN_DATA_PATH,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 7017.39 examples/s]\n",
      "Map: 100%|██████████| 437/437 [00:00<00:00, 6436.39 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 1306.58 examples/s]\n",
      "Map: 100%|██████████| 437/437 [00:00<00:00, 1324.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# custom instruct prompt start\n",
    "prompt_template = f\"Summarize this arabic text:\\n{{text}}\\n---\\nSummary:\\n{{summary}}{{eos_token}}\"\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = prompt_template.format(text=sample[\"text\"],\n",
    "                                            summary=sample[\"summary\"],\n",
    "                                            eos_token=tokenizer.eos_token)\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "train_dataset = dataset[\"train\"].map(template_dataset)\n",
    "eval_dataset = dataset[\"validation\"].map(template_dataset)\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_train_dataset = train_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(train_dataset.features)\n",
    ")\n",
    "\n",
    "\n",
    "lm_eval_dataset = eval_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(eval_dataset.features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/abounhar/.local/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if IS_CAUSAL_LM:\n",
    "    \n",
    "    if IS_SFT_TRAINING:\n",
    "\n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=MODEL_RUN_SAVE_PATH,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            learning_rate=lr,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            save_total_limit=1,\n",
    "            bf16=config['FP16_TRAINING'],\n",
    "            fp16_full_eval=config['FP16_TRAINING'],\n",
    "            logging_steps=logging_steps,\n",
    "            save_steps=save_steps,\n",
    "            eval_steps=eval_steps,\n",
    "            report_to=\"wandb\",\n",
    "            push_to_hub=False,\n",
    "            metric_for_best_model=config['METRIC_FOR_BEST_MODEL'],\n",
    "            gradient_checkpointing=True,\n",
    "            load_best_model_at_end=True,\n",
    "            optim=config['hyperparameters']['optimizer'],\n",
    "            gradient_checkpointing_kwargs={\"use_reentrant\": False} if config['hyperparameters']['USE_LORA'] else None,  # Avoids gradient issues in backprop when LoRA is set to True. # https://discuss.huggingface.co/t/how-to-combine-lora-and-gradient-checkpointing-in-whisper/50629\n",
    "        )\n",
    "        \n",
    "        # Initialize Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=lm_train_dataset,\n",
    "            eval_dataset=lm_eval_dataset,\n",
    "            data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "            compute_metrics=lambda x : compute_metrics_causal_lm(x, tokenizer),\n",
    "            preprocess_logits_for_metrics=preprocess_logits_for_metrics, # avoids OOM in eval\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print(f'[INFO] Running preprocess_function_causal_lm')\n",
    "        # Apply preprocessing\n",
    "        tokenized_dataset_train = dataset['train'].map(lambda x: preprocess_function_causal_lm(x, tokenizer), batched=True)\n",
    "        tokenized_dataset_validation = dataset['validation'].map(lambda x: preprocess_function_causal_lm(x, tokenizer), batched=True)\n",
    "        tokenized_dataset_test = dataset['test'].map(lambda x: preprocess_function_causal_lm(x, tokenizer), batched=True)\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=MODEL_RUN_SAVE_PATH,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            learning_rate=lr,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            save_total_limit=1,\n",
    "            bf16=config['FP16_TRAINING'],\n",
    "            fp16_full_eval=config['FP16_TRAINING'],\n",
    "            logging_steps=logging_steps,\n",
    "            save_steps=save_steps,\n",
    "            eval_steps=eval_steps,\n",
    "            report_to=\"wandb\",\n",
    "            push_to_hub=False,\n",
    "            metric_for_best_model=config['METRIC_FOR_BEST_MODEL'],\n",
    "            gradient_checkpointing=True,\n",
    "            load_best_model_at_end=True,\n",
    "            optim=config['hyperparameters']['optimizer'],\n",
    "            gradient_checkpointing_kwargs={\"use_reentrant\": False} if config['hyperparameters']['USE_LORA'] else None,  # Avoids gradient issues in backprop when LoRA is set to True. # https://discuss.huggingface.co/t/how-to-combine-lora-and-gradient-checkpointing-in-whisper/50629\n",
    "        )\n",
    "    \n",
    "        # Initialize Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_dataset_train,\n",
    "            eval_dataset=tokenized_dataset_validation,\n",
    "            data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "            compute_metrics=lambda x : compute_metrics_causal_lm(x, tokenizer),\n",
    "            preprocess_logits_for_metrics=preprocess_logits_for_metrics, # avoids OOM in eval\n",
    "        )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    tokenized_dataset_train = dataset['train'].map(lambda x: preprocess_function_seq2seq(x, tokenizer), batched=True)\n",
    "    tokenized_dataset_validation = dataset['validation'].map(lambda x: preprocess_function_seq2seq(x, tokenizer), batched=True)\n",
    "    tokenized_dataset_test = dataset['test'].map(lambda x: preprocess_function_seq2seq(x, tokenizer), batched=True)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=MODEL_RUN_SAVE_PATH,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        learning_rate=lr,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        save_total_limit=1,\n",
    "        predict_with_generate=True,\n",
    "        logging_steps=logging_steps,\n",
    "        save_steps=save_steps,\n",
    "        eval_steps=eval_steps,\n",
    "        report_to=\"wandb\",\n",
    "        push_to_hub=False,\n",
    "        metric_for_best_model=config['METRIC_FOR_BEST_MODEL'],\n",
    "        gradient_checkpointing=True,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset_train,\n",
    "        eval_dataset=tokenized_dataset_validation,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "        compute_metrics=lambda x : compute_metrics(x, tokenizer),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 07:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Bertscore Precision</th>\n",
       "      <th>Bertscore Recall</th>\n",
       "      <th>Bertscore F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.582500</td>\n",
       "      <td>2.602884</td>\n",
       "      <td>55.083788</td>\n",
       "      <td>38.448823</td>\n",
       "      <td>54.245476</td>\n",
       "      <td>54.241797</td>\n",
       "      <td>3.810823</td>\n",
       "      <td>18.660133</td>\n",
       "      <td>67.762839</td>\n",
       "      <td>70.961604</td>\n",
       "      <td>69.278592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.529300</td>\n",
       "      <td>2.587335</td>\n",
       "      <td>55.606804</td>\n",
       "      <td>38.910257</td>\n",
       "      <td>54.759650</td>\n",
       "      <td>54.720753</td>\n",
       "      <td>3.871437</td>\n",
       "      <td>18.798846</td>\n",
       "      <td>67.902154</td>\n",
       "      <td>71.064663</td>\n",
       "      <td>69.402197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.582000</td>\n",
       "      <td>2.582069</td>\n",
       "      <td>55.405143</td>\n",
       "      <td>38.879908</td>\n",
       "      <td>54.549638</td>\n",
       "      <td>54.533909</td>\n",
       "      <td>3.940570</td>\n",
       "      <td>18.912171</td>\n",
       "      <td>67.831755</td>\n",
       "      <td>71.012572</td>\n",
       "      <td>69.341729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.547800</td>\n",
       "      <td>2.580348</td>\n",
       "      <td>55.257497</td>\n",
       "      <td>38.818235</td>\n",
       "      <td>54.404744</td>\n",
       "      <td>54.371896</td>\n",
       "      <td>3.913501</td>\n",
       "      <td>18.850758</td>\n",
       "      <td>67.641649</td>\n",
       "      <td>70.923031</td>\n",
       "      <td>69.194091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.551100</td>\n",
       "      <td>2.580014</td>\n",
       "      <td>55.278462</td>\n",
       "      <td>38.795217</td>\n",
       "      <td>54.417087</td>\n",
       "      <td>54.376928</td>\n",
       "      <td>3.914574</td>\n",
       "      <td>18.879967</td>\n",
       "      <td>67.650611</td>\n",
       "      <td>70.893994</td>\n",
       "      <td>69.188069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=2.569189453125, metrics={'train_runtime': 468.8698, 'train_samples_per_second': 2.133, 'train_steps_per_second': 1.066, 'total_flos': 3942051575514624.0, 'train_loss': 2.569189453125, 'epoch': 1.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preparing to push to hub...\n"
     ]
    }
   ],
   "source": [
    "# Push to Hugging Face Hub\n",
    "print(\"[INFO] Preparing to push to hub...\")\n",
    "\n",
    "if config['hyperparameters']['USE_LORA']:\n",
    "    print(\"[INFO] Merging LoRA weights before pushing...\")\n",
    "    from peft import merge_and_unload\n",
    "    model = merge_and_unload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Pushing model and tokenizer to Hugging Face Hub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'BounharAbdelaziz/meh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Pushing model and tokenizer to Hugging Face Hub...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m trainer\u001b[38;5;241m.\u001b[39mpush_to_hub()\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_RUN_SAVE_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/hub.py:938\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[0;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    933\u001b[0m repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_repo(\n\u001b[1;32m    934\u001b[0m     repo_id, private\u001b[38;5;241m=\u001b[39mprivate, token\u001b[38;5;241m=\u001b[39mtoken, repo_url\u001b[38;5;241m=\u001b[39mrepo_url, organization\u001b[38;5;241m=\u001b[39morganization\n\u001b[1;32m    935\u001b[0m )\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# Create a new empty model card and eventually tag it\u001b[39;00m\n\u001b[0;32m--> 938\u001b[0m model_card \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_and_tag_model_card\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_metadata_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_metadata_errors\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_temp_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    943\u001b[0m     use_temp_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(working_dir)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/hub.py:1193\u001b[0m, in \u001b[0;36mcreate_and_tag_model_card\u001b[0;34m(repo_id, tags, token, ignore_metadata_errors)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;124;03mCreates or loads an existing model card and tags it.\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;124;03m        the process. Use it at your own risk.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;66;03m# Check if the model card is present on the remote repo\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m     model_card \u001b[38;5;241m=\u001b[39m \u001b[43mModelCard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_metadata_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_metadata_errors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;66;03m# Otherwise create a simple model card from template\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m     model_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the model card of a 🤗 transformers model that has been pushed on the Hub. This model card has been automatically generated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/huggingface_hub/repocard.py:189\u001b[0m, in \u001b[0;36mRepoCard.load\u001b[0;34m(cls, repo_id_or_path, repo_type, token, ignore_metadata_errors)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load RepoCard: path not found on disk (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Preserve newlines in the existing file.\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mcard_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(f\u001b[38;5;241m.\u001b[39mread(), ignore_metadata_errors\u001b[38;5;241m=\u001b[39mignore_metadata_errors)\n",
      "File \u001b[0;32m/usr/lib/python3.12/pathlib.py:1015\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1014\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1015\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'BounharAbdelaziz/meh'"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer locally before pushing\n",
    "# trainer.save_model(MODEL_RUN_SAVE_PATH)  # This saves the model, tokenizer, and config\n",
    "# tokenizer.save_pretrained(MODEL_RUN_SAVE_PATH)\n",
    "\n",
    "# Push to the hub\n",
    "print(\"[INFO] Pushing model and tokenizer to Hugging Face Hub...\")\n",
    "trainer.push_to_hub()\n",
    "tokenizer.push_to_hub(MODEL_RUN_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize this arabic text:\n",
      ",تشير النصوص إلى أهمية التمارين الرياضية الفترية لتحسين مستوي\n",
      "\"عبد الحكيم حذاقة-الجزائر عاشت الجزائر مرحلة مفصلية في تاريخها خلال الشهور\n",
      "العشرة الأخيرة من عام 2019، شهدت خلالها ثورة سلمية غير مسبوقة، كانت فيها قوات\n",
      "الجيش مرافقة لحراك الشعب، دون أن تراق قطرة دم. وأطاحت الثورة بنظام الرئيس عبد\n",
      "العزيز بوتفليقة بعد عشرين عاما من الحكم، وزج بأقوى رموز عهده من مسؤولين أمنيين\n",
      "وحكوميين ورجال مال وراء القضبان، حيث شهدت البلاد محاكمات تاريخية بالجملة في حق\n",
      "هؤلاء. وانتهى المخاض العسير طيلة عشرة أشهر بانتخاب رئيس جديد في 12\n",
      "ديسمبر/كانون الأول، وسط أجواء مشحونة بين المؤيدين والرافضين لشروط الانتخابات،\n",
      "حيث لا يزال جزائريون مصرين على الاحتجاج الأسبوعي. وفتحت الوفاة المفاجئة لرئيس\n",
      "الأركان الفريق أحمد قايد صالح يوم 23 ديسمبر/كانون الأول باب التكهنات حول ملامح\n",
      "العام الجديد، بالنظر إلى دور الرجل الحاسم في صناعة المرحلة الماضية.\n",
      "\n",
      "---\n",
      "Summary:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_input = \"\"\",تشير النصوص إلى أهمية التمارين الرياضية الفترية لتحسين مستوي\n",
    "\"عبد الحكيم حذاقة-الجزائر عاشت الجزائر مرحلة مفصلية في تاريخها خلال الشهور\n",
    "العشرة الأخيرة من عام 2019، شهدت خلالها ثورة سلمية غير مسبوقة، كانت فيها قوات\n",
    "الجيش مرافقة لحراك الشعب، دون أن تراق قطرة دم. وأطاحت الثورة بنظام الرئيس عبد\n",
    "العزيز بوتفليقة بعد عشرين عاما من الحكم، وزج بأقوى رموز عهده من مسؤولين أمنيين\n",
    "وحكوميين ورجال مال وراء القضبان، حيث شهدت البلاد محاكمات تاريخية بالجملة في حق\n",
    "هؤلاء. وانتهى المخاض العسير طيلة عشرة أشهر بانتخاب رئيس جديد في 12\n",
    "ديسمبر/كانون الأول، وسط أجواء مشحونة بين المؤيدين والرافضين لشروط الانتخابات،\n",
    "حيث لا يزال جزائريون مصرين على الاحتجاج الأسبوعي. وفتحت الوفاة المفاجئة لرئيس\n",
    "الأركان الفريق أحمد قايد صالح يوم 23 ديسمبر/كانون الأول باب التكهنات حول ملامح\n",
    "العام الجديد، بالنظر إلى دور الرجل الحاسم في صناعة المرحلة الماضية.\n",
    "\"\"\"\n",
    "# custom instruct prompt start\n",
    "prompt_template_test = f\"Summarize this arabic text:\\n{{text}}\\n---\\nSummary:\\n\"\n",
    "\n",
    "test_prompt = prompt_template_test.format(text=test_input)\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test summary: Summarize this arabic text:\n",
      ",تشير النصوص إلى أهمية التمارين الرياضية الفترية لتحسين مستوي\n",
      "\"عبد الحكيم حذاقة-الجزائر عاشت الجزائر مرحلة مفصلية في تاريخها خلال الشهور\n",
      "العشرة الأخيرة من عام 2019، شهدت خلالها ثورة سلمية غير مسبوقة، كانت فيها قوات\n",
      "الجيش مرافقة لحراك الشعب، دون أن تراق قطرة دم. وأطاحت الثورة بنظام الرئيس عبد\n",
      "العزيز بوتفليقة بعد عشرين عاما من الحكم، وزج بأقوى رموز عهده من مسؤولين أمنيين\n",
      "وحكوميين ورجال مال وراء القضبان، حيث شهدت البلاد محاكمات تاريخية بالجملة في حق\n",
      "هؤلاء. وانتهى المخاض العسير طيلة عشرة أشهر بانتخاب رئيس جديد في 12\n",
      "ديسمبر/كانون الأول، وسط أجواء مشحونة بين المؤيدين والرافضين لشروط الانتخابات،\n",
      "حيث لا يزال جزائريون مصرين على الاحتجاج الأسبوعي. وفتحت الوفاة المفاجئة لرئيس\n",
      "الأركان الفريق أحمد قايد صالح يوم 23 ديسمبر/كانون الأول باب التكهنات حول ملامح\n",
      "العام الجديد، بالنظر إلى دور الرجل الحاسم في صناعة المرحلة الماضية.\n",
      "\n",
      "---\n",
      "Summary:\n",
      "تعرضت الجزائر ل períادة محددة خلال السنوات الأخيرة، مع انتشار الث\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and generate\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "        **inputs,\n",
    "        # max_new_tokens=256,\n",
    "        # num_beams=4,\n",
    "        # early_stopping=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        # repetition_penalty=1.2,\n",
    "        eos_token_id=tokenizer.eos_token_id,  # Crucial for stopping\n",
    "        # pad_token_id=tokenizer.eos_token_id,  # Crucial for stopping\n",
    "        # do_sample=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# Decode the output\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# split_text = summary.split(\"### Assistant:\", 1)\n",
    "# final_pred = split_text[-1].strip() if len(split_text) > 1 else summary\n",
    "# final_pred = final_pred.replace(tokenizer.eos_token, \"\")\n",
    "        \n",
    "print(f\"Test summary: {summary}\")\n",
    "# print(f\"final_pred: {final_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_results = trainer.evaluate(\n",
    "    tokenized_dataset_test,\n",
    "    # compute_metrics=lambda eval_pred: compute_metrics_causal_lm(eval_pred, is_in_test=True)\n",
    ")\n",
    "print(f'[INFO] Results on test set: {test_results}')\n",
    "\n",
    "# # Save the model and tokenizer locally before pushing\n",
    "# trainer.save_model(MODEL_RUN_SAVE_PATH)  # This saves the model, tokenizer, and config\n",
    "# tokenizer.save_pretrained(MODEL_RUN_SAVE_PATH)\n",
    "\n",
    "# # Push to the hub\n",
    "# print(\"[INFO] Pushing model and tokenizer to Hugging Face Hub...\")\n",
    "# trainer.push_to_hub()\n",
    "# tokenizer.push_to_hub(MODEL_RUN_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('preds.csv')\n",
    "\n",
    "print(df[\"Predictions\"].iloc[0])\n",
    "\n",
    "print(df[\"Labels\"].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Labels\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
