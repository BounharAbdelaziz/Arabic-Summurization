# pretrained models
# BASE_MODEL: "bert-base-arabic"          # 111M, AutoModelForMaskedLM
# BASE_MODEL: "gpt2"               # 137M
# BASE_MODEL: "mt5-small"                 # 300M 13GB with BS 4 and GACC 16 and BF16
# BASE_MODEL: "mt5-small-SFT"                 # 300M 13GB with BS 4 and GACC 16 and BF16
# BASE_MODEL: "mt5-base"                  # 582M
# BASE_MODEL: "mt5-base-SFT"                  # 582M
# BASE_MODEL: "Qwen2.5-0.5B"              # 494M
# BASE_MODEL: "Qwen2.5-0.5B-SFT"              # 494M
# Instruct models
BASE_MODEL: "Qwen2.5-0.5B-Instruct"     # 494M

# Dataset to use
DATASET_PATH: "BounharAbdelaziz/Arabic-Synthetic-Summarization-Dataset"

# Training hyperparameters
hyperparameters:
    num_train_epochs: 6
    lr: 0.00002
    batch_size: 2
    gradient_accumulation_steps: 16
    eval_accumulation_steps: 3          # to avoid OOM in eval. Slows down eval as it offloads to CPU.
    max_grad_norm: 1.0
    warmup_steps: 100
    warmup_ratio: 0.1

    # Logging and saving
    logging_steps: 25
    save_steps: 50
    eval_steps: 50

    MAX_LEN: 2048

# To observe how the model is performing when we increase the number of training samples
MAX_TRAINING_SAMPLES: 5000

# Seed for reproducibility
SEED: 42

# metric that indicates best model
METRIC_FOR_BEST_MODEL: "rougeL"

# precision in training
FP16_TRAINING: True
# FP16_TRAINING: False

