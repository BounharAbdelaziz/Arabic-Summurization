{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/abounhar/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/infres/abounhar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/infres/abounhar/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/infres/abounhar/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "import re\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# Set CUDA device\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = \"expandable_segments:True\"\n",
    "\n",
    "# Load metrics\n",
    "rouge = load('rouge')\n",
    "bleu = load('bleu')\n",
    "meteor = load('meteor')\n",
    "bertscore = load('bertscore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_metrics_causal_lm(eval_pred, tokenizer):\n",
    "    \"\"\"Compute ROUGE and BLEU scores for evaluation.\"\"\"\n",
    "    predictions, references = eval_pred\n",
    "\n",
    "    # Clip token IDs to the valid range\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    def clip_token_ids(token_ids):\n",
    "        \"\"\"Clip token IDs to the valid range [0, vocab_size - 1].\"\"\"\n",
    "        return [min(max(token_id, 0), vocab_size - 1) for token_id in token_ids]\n",
    "\n",
    "    # Decode predictions and references\n",
    "    decoded_preds = [\n",
    "        tokenizer.decode(clip_token_ids(pred), skip_special_tokens=True)\n",
    "        for pred in predictions\n",
    "    ]\n",
    "    decoded_refs = [\n",
    "        tokenizer.decode(clip_token_ids(ref), skip_special_tokens=True)\n",
    "        for ref in references\n",
    "    ]\n",
    "    \n",
    "    # Clean summaries\n",
    "    def clean_summary(text):\n",
    "        special_tokens = [\"<|im_end|>\", \"<|assistant|>\", \"<|user|>\", \"<|system|>\"]\n",
    "        for token in special_tokens:\n",
    "            text = text.replace(token, \"\")\n",
    "        return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    pred_summaries = []\n",
    "    for pred in decoded_preds:\n",
    "        if \"<|assistant|>\" in pred:\n",
    "            summary = pred.split(\"<|assistant|>\")[-1].strip()\n",
    "            summary = clean_summary(summary)\n",
    "            pred_summaries.append(summary)\n",
    "        else:\n",
    "            summary = pred.strip()\n",
    "            summary = clean_summary(summary)\n",
    "            pred_summaries.append(summary)\n",
    "            \n",
    "    # apply the same to the references\n",
    "    ref_summaries = []\n",
    "    for ref in decoded_refs:\n",
    "        if \"<|assistant|>\" in ref:\n",
    "            summary = ref.split(\"<|assistant|>\")[-1].strip()\n",
    "            summary = clean_summary(summary)\n",
    "            ref_summaries.append(summary)\n",
    "        else:\n",
    "            summary = ref.strip()\n",
    "            summary = clean_summary(summary)\n",
    "            ref_summaries.append(summary)\n",
    "            \n",
    "    # print(f'0 - ref_summaries[0]: {ref_summaries[0]}')\n",
    "    \n",
    "    # Convert to token IDs\n",
    "    pred_token_ids = [tokenizer.encode(p, add_special_tokens=False) for p in pred_summaries]\n",
    "    ref_token_ids = [tokenizer.encode(r, add_special_tokens=False) for r in ref_summaries]\n",
    "\n",
    "    # Use the exact same metric function from training\n",
    "    eval_pred = (pred_token_ids, ref_token_ids)\n",
    "    \n",
    "    predictions, references = eval_pred\n",
    "\n",
    "    # Clip token IDs to the valid range\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    # Decode predictions and references in batches\n",
    "    decoded_preds = tokenizer.batch_decode([clip_token_ids(pred) for pred in predictions], skip_special_tokens=True)\n",
    "    decoded_refs = tokenizer.batch_decode([clip_token_ids(ref) for ref in references], skip_special_tokens=True)\n",
    "    \n",
    "    # Print decoded examples to inspect issues\n",
    "    print(f'decoded_preds[0]: {decoded_preds[0]}')\n",
    "    print(f'decoded_refs[0]: {decoded_refs[0]}')\n",
    "\n",
    "    # Compute ROUGE, BLEU and BERT scores\n",
    "    rouge_results = rouge.compute(predictions=decoded_preds, references=decoded_refs, use_stemmer=True)\n",
    "    bleu_results = bleu.compute(predictions=decoded_preds, references=decoded_refs)\n",
    "    bertscore_results = bertscore.compute(\n",
    "        predictions=decoded_preds, \n",
    "        references=decoded_refs, \n",
    "        lang='ar'\n",
    "    )\n",
    "\n",
    "    # save metrics\n",
    "    metrics = {key: rouge_results[key] * 100 for key in [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]}\n",
    "    metrics[\"bleu\"] = bleu_results[\"bleu\"] * 100\n",
    "    metrics[\"bertscore_precision\"] = sum(bertscore_results['precision']) / len(bertscore_results['precision']) * 100,\n",
    "    metrics[\"bertscore_recall\"] = sum(bertscore_results['recall']) / len(bertscore_results['recall']) * 100,\n",
    "    metrics[\"bertscore_f1\"] = sum(bertscore_results['f1']) / len(bertscore_results['f1']) * 100\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics_seq2seq(eval_pred, tokenizer):\n",
    "    preds, labels = eval_pred\n",
    "    \n",
    "    print(preds)\n",
    "    print(labels)\n",
    "    # # Clip token IDs to valid range\n",
    "    # preds = np.clip(preds, 0, tokenizer.vocab_size - 1)\n",
    "    # labels = np.clip(labels, 0, tokenizer.vocab_size - 1)\n",
    "    \n",
    "    # # Ensure labels are not masked\n",
    "    # labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Decode predictions and labels directly using batch_decode\n",
    "    text_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    text_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    print(f'text_preds[0]: {text_preds[0]}')\n",
    "    print(f'text_labels[0]: {text_labels[0]}')\n",
    "    \n",
    "    # Compute ROUGE, BLEU and BERT scores\n",
    "    rouge_results = rouge.compute(predictions=text_preds, references=text_labels, use_stemmer=True)\n",
    "    bleu_results = bleu.compute(predictions=text_preds, references=text_labels)\n",
    "    bertscore_results = bertscore.compute(\n",
    "        predictions=text_preds, \n",
    "        references=text_labels, \n",
    "        lang='ar'\n",
    "    )\n",
    "\n",
    "    # save metrics\n",
    "    metrics = {key: rouge_results[key] * 100 for key in [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]}\n",
    "    metrics[\"bleu\"] = bleu_results[\"bleu\"] * 100\n",
    "    metrics[\"bertscore_precision\"] = sum(bertscore_results['precision']) / len(bertscore_results['precision']) * 100,\n",
    "    metrics[\"bertscore_recall\"] = sum(bertscore_results['recall']) / len(bertscore_results['recall']) * 100,\n",
    "    metrics[\"bertscore_f1\"] = sum(bertscore_results['f1']) / len(bertscore_results['f1']) * 100\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch summarization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dataset(dataset, model, tokenizer, model_name, is_causal, max_length=1024, max_new_tokens=256, batch_size=16, device=\"cuda\"):\n",
    "    \"\"\"Summarize all texts in the dataset using the trained model.\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Summarize in batches\n",
    "    summaries = []\n",
    "    \n",
    "    # Get the actual text column name from the dataset\n",
    "    text_column = 'text'  # adjust if your column name is different\n",
    "    \n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        # Get a batch of examples\n",
    "        batch = dataset[i:i + batch_size]\n",
    "        \n",
    "        # Extract the text content properly\n",
    "        batch_texts = batch[text_column]\n",
    "        \n",
    "        # causal models were trained in SFT mode with chat template\n",
    "        if is_causal:\n",
    "            # Prepare the messages for the model using the tokenizer's chat template\n",
    "            messages = [\n",
    "                [{\"role\": \"user\", \"content\": text}] for text in batch_texts\n",
    "            ]\n",
    "            \n",
    "            # Apply the chat template\n",
    "            input_ids = tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                truncation=True,\n",
    "                max_length=max_length,  # adjust based on your model's context window\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            )\n",
    "            # Create attention mask based on non-zero tokens\n",
    "            attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "        else:\n",
    "            \n",
    "            # Tokenize with explicit padding to max_length\n",
    "            inputs = tokenizer(batch_texts, max_length=max_length, truncation=True, padding='max_length')\n",
    "            input_ids = torch.LongTensor(inputs['input_ids'])\n",
    "            attention_mask = torch.LongTensor(inputs['attention_mask'])\n",
    "\n",
    "            # Fix 1: Add explicit position IDs clamping\n",
    "            position_ids = torch.arange(0, input_ids.size(-1), dtype=torch.long, device=device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "\n",
    "            # Access max_position_embeddings directly from the model's config\n",
    "            max_position_embeddings = max_length\n",
    "\n",
    "            position_ids = position_ids.clamp(max=max_position_embeddings - 1)\n",
    "\n",
    "            # Fix 2: Ensure correct truncation length\n",
    "            truncation_length = max_position_embeddings\n",
    "            if input_ids.shape[1] > truncation_length:\n",
    "                input_ids = input_ids[:, :truncation_length]\n",
    "                attention_mask = attention_mask[:, :truncation_length]\n",
    "\n",
    "\n",
    "        \n",
    "        # Move tensors to device\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        \n",
    "        # Summarize the batch\n",
    "        batch_summaries = summarize_batch(\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            max_new_tokens,\n",
    "            device\n",
    "        )\n",
    "        \n",
    "        # Save summaries\n",
    "        summaries.extend(batch_summaries)\n",
    "        \n",
    "    # Add summaries to the dataset\n",
    "    dataset = dataset.add_column(f\"summary_{model_name}\", summaries)\n",
    "    return dataset\n",
    "\n",
    "def summarize_batch(input_ids, attention_mask, model, tokenizer, max_new_tokens, device):\n",
    "    generation_config = model.generation_config\n",
    "    \n",
    "    # Generate summaries\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            bos_token_id=generation_config.bos_token_id,\n",
    "            eos_token_id=generation_config.eos_token_id,\n",
    "            pad_token_id=generation_config.pad_token_id,\n",
    "            # num_beams=3,\n",
    "            # do_sample=True,\n",
    "            # temperature=0.7,\n",
    "            # top_k=50,\n",
    "            # top_p=0.95\n",
    "        )\n",
    "    \n",
    "    # Decode the generated outputs\n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    # Extract summaries (text after <|assistant|>)\n",
    "    summaries = []\n",
    "    for text in generated_texts:\n",
    "        if \"<|assistant|>\" in text:\n",
    "            summaries.append(text.split(\"<|assistant|>\")[-1].strip())\n",
    "        else:\n",
    "            summaries.append(text.strip())\n",
    "    \n",
    "    # Clean summaries\n",
    "    def clean_summary(text):\n",
    "        special_tokens = [\"<|im_end|>\", \"<|assistant|>\", \"<|user|>\", \"<|system|>\"]\n",
    "        for token in special_tokens:\n",
    "            text = text.replace(token, \"\")\n",
    "        return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    cleaned_summaries = [clean_summary(summary) for summary in summaries]\n",
    "    \n",
    "    return cleaned_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat template creation and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversation(example):\n",
    "    \"\"\"\n",
    "    Transform the dataset into a conversational format.\n",
    "    The user provides the text, and the assistant provides the summary.\n",
    "    \"\"\"\n",
    "    # Create a conversation with user and assistant roles\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": example[\"text\"]},  # User provides the text\n",
    "    ]\n",
    "    # Return the conversation as a dictionary\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def apply_chat_template(example, tokenizer):\n",
    "    \"\"\" Apply the chat template to the dataset. \"\"\"\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)\n",
    "    return example\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples['text'], padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DICT = {\n",
    "    ##############################                 QWEN models                #############################################\n",
    "    # full mixed precision finetuning\n",
    "    \"BounharAbdelaziz/Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5\": {\n",
    "        \"batch_size\": 64,\n",
    "        \"is_causal\": True,\n",
    "        \"max_len\": 2048,\n",
    "    },\n",
    "    \"BounharAbdelaziz/Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5\":{\n",
    "        \"batch_size\": 64,\n",
    "        \"is_causal\": True,\n",
    "        \"max_len\": 2048,\n",
    "    },\n",
    "    # LoRA finetuned models\n",
    "    \"BounharAbdelaziz/Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-128-a-64-v5\": {\n",
    "        \"batch_size\": 16,\n",
    "        \"is_causal\": True,\n",
    "        \"max_len\": 2048,\n",
    "    },\n",
    "    \"BounharAbdelaziz/Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-256-a-128-v5\":{\n",
    "        \"batch_size\": 16,\n",
    "        \"is_causal\": True,\n",
    "        \"max_len\": 2048,\n",
    "    },\n",
    "    ###############################                 Falcon models                #############################################\n",
    "    # full mixed precision finetuning\n",
    "    \"BounharAbdelaziz/Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5\": {\n",
    "        \"batch_size\": 32,\n",
    "        \"is_causal\": True,\n",
    "        \"max_len\": 1024,\n",
    "    },\n",
    "    \"BounharAbdelaziz/Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5\":{\n",
    "        \"batch_size\": 32,\n",
    "        \"is_causal\": True,\n",
    "        \"max_len\": 1024,\n",
    "    },\n",
    "    # LoRA finetuned models\n",
    "    \"BounharAbdelaziz/Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-128-a-64-v5\": {\n",
    "        \"batch_size\": 32,\n",
    "        \"is_causal\": True,\n",
    "        \"max_len\": 1024,\n",
    "    },\n",
    "    \"BounharAbdelaziz/Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-256-a-128-v5\":{\n",
    "        \"batch_size\": 32,\n",
    "        \"is_causal\": True,\n",
    "        \"max_len\": 1024,\n",
    "    },\n",
    "    ###############################                 mT5 models                #############################################\n",
    "    # full mixed precision finetuning\n",
    "    \"BounharAbdelaziz/mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5\": {\n",
    "        \"batch_size\": 128,\n",
    "        \"is_causal\": False,\n",
    "        \"max_len\": 1024,\n",
    "    },\n",
    "    \"BounharAbdelaziz/mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5\":{\n",
    "        \"batch_size\": 128,\n",
    "        \"is_causal\": False,\n",
    "        \"max_len\": 1024,\n",
    "    },\n",
    "    \"BounharAbdelaziz/mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5\": {\n",
    "        \"batch_size\": 128,\n",
    "        \"is_causal\": False,\n",
    "        \"max_len\": 1024,\n",
    "    },\n",
    "    \"BounharAbdelaziz/mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5\": {\n",
    "        \"batch_size\": 128,\n",
    "        \"is_causal\": False,\n",
    "        \"max_len\": 1024,\n",
    "    },\n",
    "    # ###############################                 GPT2 models                #############################################\n",
    "    # # full mixed precision finetuning\n",
    "    # \"BounharAbdelaziz/gpt2-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-SFT-mx-1024-v5\": {\n",
    "    #     \"batch_size\": 1,\n",
    "    #     \"is_causal\": True,\n",
    "    #     \"max_len\": 1024,\n",
    "    # },\n",
    "    # \"BounharAbdelaziz/gpt2-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-8-gnm-1.0-FP16-SFT-mx-1024-v5\":{\n",
    "    #     \"batch_size\": 1,\n",
    "    #     \"is_causal\": True,\n",
    "    #     \"max_len\": 1024,\n",
    "    # },\n",
    "    # \"BounharAbdelaziz/gpt2-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-mx-1024-v5\": {\n",
    "    #     \"batch_size\": 1,\n",
    "    #     \"is_causal\": True,\n",
    "    #     \"max_len\": 1024,\n",
    "    # },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "eval_dataset = load_dataset(\"BounharAbdelaziz/Arabic-Synthetic-Summarization-Dataset-Filtered\", split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'summary', 'summary_model_name', 'tokenizer_name', 'dataset_source', 'sequence_length'],\n",
       "    num_rows: 444\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat template for SFT models\n",
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "# precision\n",
    "torch_dtype = torch.float16\n",
    "\n",
    "# inference device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cput\"\n",
    "\n",
    "# maximum number of tokens for generate()\n",
    "MAX_NEW_TOKENS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and empty dataframe to store the results\n",
    "metrics_df = pd.DataFrame()\n",
    "\n",
    "for model_path, config in MODELS_DICT.items():\n",
    "    \n",
    "    BATCH_SIZE = config['batch_size']\n",
    "    IS_CAUSAL_LM = config['is_causal']\n",
    "    MAX_LEN = config['max_len']\n",
    "\n",
    "    if IS_CAUSAL_LM:\n",
    "        # load model\n",
    "        if \"gpt2\" in model_path:\n",
    "            # GPT2 models surprisingly don't work. They raise a cuda error that I wasn't able to debug.\n",
    "            # With the fact that other models already performed better on the evaluation set, I didn't include them in this work.\n",
    "            # Also, they generate a similar error when trained with a batch size larger than 1! \n",
    "            model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch_dtype)\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch_dtype).to(\"cuda\")\n",
    "        model.use_cache = True\n",
    "        \n",
    "        # load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            \n",
    "        # Set chat template\n",
    "        tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n",
    "        \n",
    "        if \"gpt2\" in model_path:\n",
    "            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        \n",
    "    else:\n",
    "        # load model\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_path, torch_dtype=torch_dtype).to(\"cuda\")\n",
    "        model.use_cache = True\n",
    "        \n",
    "        # load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # set padding side deppending on model type\n",
    "    tokenizer.padding_side= 'left' if IS_CAUSAL_LM else 'right'\n",
    "\n",
    "    # Set reasonable default for models without max length\n",
    "    tokenizer.model_max_length = MAX_LEN\n",
    "\n",
    "    # Set pad_token_id equal to the eos_token_id if not set\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "    # get model name -> used in column name for saving summaries predictions in dataset\n",
    "    model_name = model_path.split('/')[-1].strip()\n",
    "    \n",
    "    # run summarization and saving of dataset\n",
    "    eval_dataset = summarize_dataset(eval_dataset, model, tokenizer, model_name, is_causal=IS_CAUSAL_LM, max_length=MAX_LEN, max_new_tokens=MAX_NEW_TOKENS, batch_size=BATCH_SIZE, device=device)\n",
    "    \n",
    "    # get predictions and references\n",
    "    predictions = eval_dataset[f\"summary_{model_path.split('/')[-1]}\"]\n",
    "    references = eval_dataset[\"summary\"]\n",
    "\n",
    "    # Convert to token IDs like during training\n",
    "    pred_token_ids = [tokenizer.encode(p) for p in predictions]\n",
    "    ref_token_ids = [tokenizer.encode(r) for r in references]\n",
    "\n",
    "    # Use the exact same metric function from training\n",
    "    eval_pred = (pred_token_ids, ref_token_ids)\n",
    "    \n",
    "    # compute metrics\n",
    "    if IS_CAUSAL_LM:\n",
    "        metrics = compute_metrics_causal_lm(eval_pred, tokenizer)\n",
    "    else:\n",
    "        metrics = compute_metrics_seq2seq(eval_pred, tokenizer)\n",
    "        \n",
    "    print(metrics)\n",
    "    \n",
    "    # save metrics\n",
    "    metrics_df[model_name] = metrics\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'summary', 'summary_model_name', 'tokenizer_name', 'dataset_source', 'sequence_length', 'summary_Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5', 'summary_Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5', 'summary_Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-128-a-64-v5', 'summary_Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-256-a-128-v5', 'summary_Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5', 'summary_Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5', 'summary_Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-128-a-64-v5', 'summary_Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-256-a-128-v5', 'summary_mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5', 'summary_mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5', 'summary_mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5', 'summary_mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5'],\n",
       "    num_rows: 444\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = metrics_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "      <th>bleu</th>\n",
       "      <th>bertscore_precision</th>\n",
       "      <th>bertscore_recall</th>\n",
       "      <th>bertscore_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5</th>\n",
       "      <td>10.154739</td>\n",
       "      <td>4.39905</td>\n",
       "      <td>10.042021</td>\n",
       "      <td>10.094154</td>\n",
       "      <td>2.364585</td>\n",
       "      <td>(65.95857325974886,)</td>\n",
       "      <td>(72.82558612458341,)</td>\n",
       "      <td>69.142793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5</th>\n",
       "      <td>8.773724</td>\n",
       "      <td>3.949345</td>\n",
       "      <td>8.650295</td>\n",
       "      <td>8.765712</td>\n",
       "      <td>2.038522</td>\n",
       "      <td>(73.20954905705409,)</td>\n",
       "      <td>(73.9764558570879,)</td>\n",
       "      <td>73.494251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-128-a-64-v5</th>\n",
       "      <td>13.796634</td>\n",
       "      <td>6.548504</td>\n",
       "      <td>13.531252</td>\n",
       "      <td>13.649635</td>\n",
       "      <td>2.952568</td>\n",
       "      <td>(75.026213001829,)</td>\n",
       "      <td>(76.43683350032514,)</td>\n",
       "      <td>75.618538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-256-a-128-v5</th>\n",
       "      <td>13.618346</td>\n",
       "      <td>6.298045</td>\n",
       "      <td>13.241807</td>\n",
       "      <td>13.384698</td>\n",
       "      <td>2.987951</td>\n",
       "      <td>(75.16128604745006,)</td>\n",
       "      <td>(76.52390929492744,)</td>\n",
       "      <td>75.737516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5</th>\n",
       "      <td>12.72673</td>\n",
       "      <td>5.657916</td>\n",
       "      <td>12.273076</td>\n",
       "      <td>12.344337</td>\n",
       "      <td>2.961368</td>\n",
       "      <td>(68.77494174483661,)</td>\n",
       "      <td>(76.1501824130883,)</td>\n",
       "      <td>72.178532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5</th>\n",
       "      <td>13.231652</td>\n",
       "      <td>5.493227</td>\n",
       "      <td>12.838139</td>\n",
       "      <td>12.840043</td>\n",
       "      <td>2.95402</td>\n",
       "      <td>(68.61420049592182,)</td>\n",
       "      <td>(76.11227748361794,)</td>\n",
       "      <td>72.074321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-128-a-64-v5</th>\n",
       "      <td>13.088594</td>\n",
       "      <td>5.741706</td>\n",
       "      <td>12.63974</td>\n",
       "      <td>12.649485</td>\n",
       "      <td>3.057522</td>\n",
       "      <td>(68.65262965764012,)</td>\n",
       "      <td>(76.27777336417017,)</td>\n",
       "      <td>72.178872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-256-a-128-v5</th>\n",
       "      <td>13.387896</td>\n",
       "      <td>6.16999</td>\n",
       "      <td>12.880398</td>\n",
       "      <td>12.948814</td>\n",
       "      <td>3.018385</td>\n",
       "      <td>(68.92468658116486,)</td>\n",
       "      <td>(76.25782343449893,)</td>\n",
       "      <td>72.30786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5</th>\n",
       "      <td>7.060375</td>\n",
       "      <td>1.708262</td>\n",
       "      <td>7.040035</td>\n",
       "      <td>7.073086</td>\n",
       "      <td>4.883842</td>\n",
       "      <td>(69.3450996467659,)</td>\n",
       "      <td>(69.0343491128973,)</td>\n",
       "      <td>69.115218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5</th>\n",
       "      <td>5.048542</td>\n",
       "      <td>0.79461</td>\n",
       "      <td>4.940549</td>\n",
       "      <td>5.043148</td>\n",
       "      <td>4.353375</td>\n",
       "      <td>(68.84305755028853,)</td>\n",
       "      <td>(67.02528249573064,)</td>\n",
       "      <td>67.814629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5</th>\n",
       "      <td>4.416399</td>\n",
       "      <td>1.109874</td>\n",
       "      <td>4.37508</td>\n",
       "      <td>4.480316</td>\n",
       "      <td>3.042328</td>\n",
       "      <td>(64.77119280948294,)</td>\n",
       "      <td>(66.90649196237058,)</td>\n",
       "      <td>65.685738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5</th>\n",
       "      <td>1.649588</td>\n",
       "      <td>0.529154</td>\n",
       "      <td>1.6513</td>\n",
       "      <td>1.625759</td>\n",
       "      <td>0.668181</td>\n",
       "      <td>(67.4000255226552,)</td>\n",
       "      <td>(61.06672634681066,)</td>\n",
       "      <td>63.931905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       rouge1    rouge2  \\\n",
       "Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32...  10.154739   4.39905   \n",
       "Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0....   8.773724  3.949345   \n",
       "Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-...  13.796634  6.548504   \n",
       "Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-...  13.618346  6.298045   \n",
       "Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gacc...   12.72673  5.657916   \n",
       "Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-...  13.231652  5.493227   \n",
       "Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-...  13.088594  5.741706   \n",
       "Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-...  13.387896   6.16999   \n",
       "mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-...   7.060375  1.708262   \n",
       "mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm...   5.048542   0.79461   \n",
       "mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm...   4.416399  1.109874   \n",
       "mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-...   1.649588  0.529154   \n",
       "\n",
       "                                                       rougeL  rougeLsum  \\\n",
       "Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32...  10.042021  10.094154   \n",
       "Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0....   8.650295   8.765712   \n",
       "Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-...  13.531252  13.649635   \n",
       "Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-...  13.241807  13.384698   \n",
       "Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gacc...  12.273076  12.344337   \n",
       "Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-...  12.838139  12.840043   \n",
       "Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-...   12.63974  12.649485   \n",
       "Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-...  12.880398  12.948814   \n",
       "mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-...   7.040035   7.073086   \n",
       "mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm...   4.940549   5.043148   \n",
       "mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm...    4.37508   4.480316   \n",
       "mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-...     1.6513   1.625759   \n",
       "\n",
       "                                                        bleu  \\\n",
       "Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32...  2.364585   \n",
       "Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0....  2.038522   \n",
       "Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-...  2.952568   \n",
       "Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-...  2.987951   \n",
       "Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gacc...  2.961368   \n",
       "Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-...   2.95402   \n",
       "Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-...  3.057522   \n",
       "Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-...  3.018385   \n",
       "mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-...  4.883842   \n",
       "mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm...  4.353375   \n",
       "mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm...  3.042328   \n",
       "mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-...  0.668181   \n",
       "\n",
       "                                                     bertscore_precision  \\\n",
       "Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32...  (65.95857325974886,)   \n",
       "Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0....  (73.20954905705409,)   \n",
       "Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-...    (75.026213001829,)   \n",
       "Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-...  (75.16128604745006,)   \n",
       "Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gacc...  (68.77494174483661,)   \n",
       "Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-...  (68.61420049592182,)   \n",
       "Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-...  (68.65262965764012,)   \n",
       "Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-...  (68.92468658116486,)   \n",
       "mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-...   (69.3450996467659,)   \n",
       "mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm...  (68.84305755028853,)   \n",
       "mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm...  (64.77119280948294,)   \n",
       "mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-...   (67.4000255226552,)   \n",
       "\n",
       "                                                        bertscore_recall  \\\n",
       "Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32...  (72.82558612458341,)   \n",
       "Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0....   (73.9764558570879,)   \n",
       "Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-...  (76.43683350032514,)   \n",
       "Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-...  (76.52390929492744,)   \n",
       "Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gacc...   (76.1501824130883,)   \n",
       "Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-...  (76.11227748361794,)   \n",
       "Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-...  (76.27777336417017,)   \n",
       "Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-...  (76.25782343449893,)   \n",
       "mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-...   (69.0343491128973,)   \n",
       "mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm...  (67.02528249573064,)   \n",
       "mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm...  (66.90649196237058,)   \n",
       "mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-...  (61.06672634681066,)   \n",
       "\n",
       "                                                   bertscore_f1  \n",
       "Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32...    69.142793  \n",
       "Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0....    73.494251  \n",
       "Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-...    75.618538  \n",
       "Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-...    75.737516  \n",
       "Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gacc...    72.178532  \n",
       "Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-...    72.074321  \n",
       "Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-...    72.178872  \n",
       "Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-...     72.30786  \n",
       "mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-...    69.115218  \n",
       "mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm...    67.814629  \n",
       "mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm...    65.685738  \n",
       "mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-...    63.931905  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv('test_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.from_pandas(metrics_df).push_to_hub(\"BounharAbdelaziz/Arabic-Summarization-Eval-Metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'summary', 'summary_model_name', 'tokenizer_name', 'dataset_source', 'sequence_length', 'summary_Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5', 'summary_Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5', 'summary_Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-128-a-64-v5', 'summary_Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-256-a-128-v5', 'summary_Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5', 'summary_Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5', 'summary_Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-128-a-64-v5', 'summary_Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-256-a-128-v5', 'summary_mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5', 'summary_mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5', 'summary_mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5', 'summary_mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5'],\n",
       "    num_rows: 444\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 22.91ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.70s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/BounharAbdelaziz/Arabic-Synthetic-Summarization-Dataset-Eval/commit/02ecbfe1e3d1b1433ea6f7bbe023473a4bd56b39', commit_message='Pushed model predictions.', commit_description='', oid='02ecbfe1e3d1b1433ea6f7bbe023473a4bd56b39', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/BounharAbdelaziz/Arabic-Synthetic-Summarization-Dataset-Eval', endpoint='https://huggingface.co', repo_type='dataset', repo_id='BounharAbdelaziz/Arabic-Synthetic-Summarization-Dataset-Eval'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.push_to_hub(\"BounharAbdelaziz/Arabic-Synthetic-Summarization-Dataset-Eval\", commit_message=\"Pushed model predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  5.45ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12320738"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.to_csv('test_summaries.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM as a Judge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality_batch(model, tokenizer, texts, summaries, system_prompt, batch_size, max_new_tokens=512):\n",
    "    \"\"\"\n",
    "    This function evaluates the quality of summaries for a batch of queries using the model.\n",
    "    \n",
    "    Args:\n",
    "        model: the preloaded model to use\n",
    "        tokenizer: the preloaded tokenizer to use\n",
    "        texts: list of input texts (for context)\n",
    "        summaries: list of model summaries to evaluate\n",
    "        system_prompt: the system prompt for summarization quality evaluation\n",
    "        batch_size: number of queries to process in parallel\n",
    "        max_new_tokens: maximum number of new tokens to generate (not used in this case)\n",
    "        \n",
    "    Returns:\n",
    "        list of dictionaries with quality scores and model names\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Results will store the evaluation scores\n",
    "    results = []\n",
    "    \n",
    "    # Use torch.no_grad() to disable gradient computation\n",
    "    with torch.no_grad():\n",
    "        # Iterate through queries and summaries in batches\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_summaries = summaries[i:i + batch_size]\n",
    "            \n",
    "            # Prepare messages for the entire batch\n",
    "            batch_messages = [\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"Text: {text}\\nSummary: {summary}\"}\n",
    "                ] for text, summary in zip(batch_texts, batch_summaries)\n",
    "            ]\n",
    "            \n",
    "            # Tokenize the batch with left padding\n",
    "            batch_texts = [\n",
    "                tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                ) for messages in batch_messages\n",
    "            ]\n",
    "            \n",
    "            model_inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, padding_side='left').to(model.device)\n",
    "            \n",
    "            # Generate evaluations for the batch\n",
    "            generated_ids = model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=max_new_tokens\n",
    "            )\n",
    "            \n",
    "            # Decode the generated tokens (which contains the score)\n",
    "            generated_ids = [\n",
    "                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "            \n",
    "            # Batch decode the results\n",
    "            quality_scores = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # Convert the quality scores to numeric values (assuming LLM outputs valid integers like \"0\", \"1\", or \"2\")\n",
    "            for score in quality_scores:\n",
    "                try:\n",
    "                    results.append(int(score.strip()))\n",
    "                except ValueError:\n",
    "                    results.append(-1)  # Error handling if the response is invalid\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compute_average_scores(dataset, model_names):\n",
    "    \"\"\"\n",
    "    Compute the average scores for each model in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset containing quality scores for each model\n",
    "        model_names: List of model names whose average score needs to be computed\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary of average scores per model\n",
    "    \"\"\"\n",
    "    avg_scores = {}\n",
    "    for model_name in model_names:\n",
    "        quality_scores = dataset[f'quality_score_summary_{model_name}']\n",
    "        avg_scores[model_name] = sum(quality_scores) / len(quality_scores) if len(quality_scores) > 0 else 0\n",
    "        \n",
    "    # Convert to DataFrame\n",
    "    avg_scores = pd.DataFrame(list(avg_scores.items()), columns=['Model', 'LLM Average Score'])\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "[INFO] Loaded configuration:\n",
      "--------------------------------------------------\n",
      "[INFO] Using LLM Judge: Qwen/Qwen2.5-7B-Instruct-AWQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.73s/it]\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# get configuration\n",
    "with open('eval_config.yaml') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "print('-'*50)\n",
    "print(\"[INFO] Loaded configuration:\")\n",
    "print('-'*50)\n",
    "\n",
    "eval_dataset = load_dataset('BounharAbdelaziz/Arabic-Synthetic-Summarization-Dataset-Eval', split='test')\n",
    "\n",
    "# set the model to use\n",
    "JUDGE_MODEL_NAME = config['JUDGE_MODEL_NAME']\n",
    "print(f'[INFO] Using LLM Judge: {JUDGE_MODEL_NAME}')\n",
    "\n",
    "# set the system prompt for quality evaluation\n",
    "SYSTEM_PROMPT = config['SYSTEM_PROMPT']\n",
    "\n",
    "# Batched processing\n",
    "batch_size = config['BATCH_SIZE']\n",
    "\n",
    "# load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    JUDGE_MODEL_NAME,\n",
    "    torch_dtype=torch.float16,  # Use float16 for faster inference\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",  # Enable Flash Attention 2 for faster inference\n",
    ")\n",
    "# load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(JUDGE_MODEL_NAME)\n",
    "\n",
    "# Set padding side to left for decoder-only models\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'summary', 'summary_model_name', 'tokenizer_name', 'dataset_source', 'sequence_length', 'summary_Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5', 'summary_Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5', 'summary_Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-128-a-64-v5', 'summary_Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-256-a-128-v5', 'summary_Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5', 'summary_Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5', 'summary_Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-128-a-64-v5', 'summary_Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-256-a-128-v5', 'summary_mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5', 'summary_mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5', 'summary_mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5', 'summary_mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5'],\n",
       "    num_rows: 444\n",
       "})"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_col: summary_Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating summary_Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5:   0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating summary_Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5: 100%|██████████| 28/28 [03:37<00:00,  7.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_col: summary_Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating summary_Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5: 100%|██████████| 28/28 [03:19<00:00,  7.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_col: summary_Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-128-a-64-v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating summary_Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-128-a-64-v5: 100%|██████████| 28/28 [03:20<00:00,  7.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_col: summary_Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-256-a-128-v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating summary_Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-256-a-128-v5: 100%|██████████| 28/28 [03:22<00:00,  7.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_col: summary_Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating summary_Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5: 100%|██████████| 28/28 [02:28<00:00,  5.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_col: summary_Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating summary_Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5: 100%|██████████| 28/28 [02:29<00:00,  5.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_col: summary_Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-128-a-64-v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating summary_Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-128-a-64-v5: 100%|██████████| 28/28 [02:32<00:00,  5.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_col: summary_Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-256-a-128-v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating summary_Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-256-a-128-v5: 100%|██████████| 28/28 [02:27<00:00,  5.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_col: summary_mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating summary_mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5: 100%|██████████| 28/28 [02:09<00:00,  4.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_col: summary_mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating summary_mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5: 100%|██████████| 28/28 [02:04<00:00,  4.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_col: summary_mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating summary_mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5: 100%|██████████| 28/28 [02:12<00:00,  4.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_col: summary_mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating summary_mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5: 100%|██████████| 28/28 [02:02<00:00,  4.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# dataset that we will update\n",
    "updated_dataset = DatasetDict()\n",
    "\n",
    "model_names = []  # To track the model names\n",
    "\n",
    "# Prepare texts and summaries for batched processing\n",
    "texts = eval_dataset['text']\n",
    "\n",
    "# Process each model's summaries (you may iterate through all model summaries here)\n",
    "for model_col in eval_dataset.column_names:\n",
    "    if model_col.startswith(\"summary_\") and model_col != \"summary_model_name\":\n",
    "        print(f'model_col: {model_col}')\n",
    "        summaries = eval_dataset[model_col]\n",
    "        \n",
    "        model_name = model_col.replace(\"summary_\", \"\")\n",
    "        model_names.append(model_name)  # Store the model name for later\n",
    "        \n",
    "        # Process in batches with progress bar\n",
    "        updated_results = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=f\"Evaluating {model_col}\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_summaries = summaries[i:i + batch_size]\n",
    "            batch_scores = evaluate_quality_batch(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                batch_texts,\n",
    "                batch_summaries,\n",
    "                SYSTEM_PROMPT,\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "            updated_results.extend(batch_scores)\n",
    "        \n",
    "        # Save results in the dataset\n",
    "        eval_dataset = eval_dataset.add_column(f\"quality_score_{model_col}\", updated_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'summary', 'summary_model_name', 'tokenizer_name', 'dataset_source', 'sequence_length', 'summary_Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5', 'summary_Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5', 'summary_Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-128-a-64-v5', 'summary_Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-256-a-128-v5', 'summary_Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5', 'summary_Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5', 'summary_Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-128-a-64-v5', 'summary_Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-256-a-128-v5', 'summary_mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5', 'summary_mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5', 'summary_mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5', 'summary_mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5', 'quality_score_summary_Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5', 'quality_score_summary_Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5', 'quality_score_summary_Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-128-a-64-v5', 'quality_score_summary_Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-256-a-128-v5', 'quality_score_summary_Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5', 'quality_score_summary_Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5', 'quality_score_summary_Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-128-a-64-v5', 'quality_score_summary_Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-256-a-128-v5', 'quality_score_summary_mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5', 'quality_score_summary_mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5', 'quality_score_summary_mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5', 'quality_score_summary_mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5'],\n",
       "    num_rows: 444\n",
       "})"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5',\n",
       " 'Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-v5',\n",
       " 'Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-128-a-64-v5',\n",
       " 'Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-256-a-128-v5',\n",
       " 'Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5',\n",
       " 'Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-v5',\n",
       " 'Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-128-a-64-v5',\n",
       " 'Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-256-a-128-v5',\n",
       " 'mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5',\n",
       " 'mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5',\n",
       " 'mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5',\n",
       " 'mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm-1.0-FP16-mx-1024-v5']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Average scores per model:\n",
      "                                                Model  LLM Average Score\n",
      "0   Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-3...           1.168919\n",
      "1   Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0...           1.074324\n",
      "2   Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1...           1.637387\n",
      "3   Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1...           1.632883\n",
      "4   Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gac...           1.403153\n",
      "5   Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1...           1.398649\n",
      "6   Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1...           1.443694\n",
      "7   Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1...           1.436937\n",
      "8   mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm...           0.520270\n",
      "9   mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gn...           0.229730\n",
      "10  mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gn...           0.189189\n",
      "11  mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm...           0.036036\n"
     ]
    }
   ],
   "source": [
    "# Compute and display average scores for each model\n",
    "avg_scores = compute_average_scores(eval_dataset, model_names)\n",
    "print(\"[INFO] Average scores per model:\")\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>LLM Average Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-3...</td>\n",
       "      <td>1.168919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0...</td>\n",
       "      <td>1.074324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1...</td>\n",
       "      <td>1.637387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1...</td>\n",
       "      <td>1.632883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gac...</td>\n",
       "      <td>1.403153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1...</td>\n",
       "      <td>1.398649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1...</td>\n",
       "      <td>1.443694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1...</td>\n",
       "      <td>1.436937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm...</td>\n",
       "      <td>0.520270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gn...</td>\n",
       "      <td>0.229730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gn...</td>\n",
       "      <td>0.189189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm...</td>\n",
       "      <td>0.036036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Model  LLM Average Score\n",
       "0   Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-3...           1.168919\n",
       "1   Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0...           1.074324\n",
       "2   Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1...           1.637387\n",
       "3   Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1...           1.632883\n",
       "4   Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gac...           1.403153\n",
       "5   Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1...           1.398649\n",
       "6   Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1...           1.443694\n",
       "7   Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1...           1.436937\n",
       "8   mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm...           0.520270\n",
       "9   mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gn...           0.229730\n",
       "10  mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gn...           0.189189\n",
       "11  mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm...           0.036036"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1960.87ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/BounharAbdelaziz/Arabic-Summarization-Eval-LLM-as-a-Judge/commit/8a61df3a3271ef62bb900b1e8a0c2d6f1d97fc50', commit_message='LLM as a judge evaluation scores.', commit_description='', oid='8a61df3a3271ef62bb900b1e8a0c2d6f1d97fc50', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/BounharAbdelaziz/Arabic-Summarization-Eval-LLM-as-a-Judge', endpoint='https://huggingface.co', repo_type='dataset', repo_id='BounharAbdelaziz/Arabic-Summarization-Eval-LLM-as-a-Judge'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.from_pandas(avg_scores).push_to_hub(\"BounharAbdelaziz/Arabic-Summarization-Eval-LLM-as-a-Judge\", private=True, commit_message=\"LLM as a judge evaluation scores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Quantization\n",
    "\n",
    "As the best performing model is a 3B model, we quantize it and verify the scores of the quantized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"BounharAbdelaziz/Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-128-a-64-v5\"\n",
    "quant_path = 'Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-128-a-64-v5-awq'\n",
    "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 13 files: 100%|██████████| 13/13 [00:00<00:00, 115033.65it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.41it/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
      "AWQ: 100%|██████████| 36/36 [09:37<00:00, 16.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = AutoAWQForCausalLM.from_pretrained(\n",
    "    best_model_path, **{\"low_cpu_mem_usage\": True, \"use_cache\": False}\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(best_model_path, trust_remote_code=True)\n",
    "\n",
    "# Quantize\n",
    "model.quantize(tokenizer, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-128-a-64-v5-awq/tokenizer_config.json',\n",
       " 'Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-128-a-64-v5-awq/special_tokens_map.json',\n",
       " 'Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-128-a-64-v5-awq/vocab.json',\n",
       " 'Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-128-a-64-v5-awq/merges.txt',\n",
       " 'Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-128-a-64-v5-awq/added_tokens.json',\n",
       " 'Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-128-a-64-v5-awq/tokenizer.json')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save quantized model\n",
    "model.save_quantized(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "tokenizer.json: 100%|██████████| 11.4M/11.4M [00:01<00:00, 9.68MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Push using `huggingface_hub`\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "hub_path = \"BounharAbdelaziz/Qwen2.5-3B-Instruct-Summarizer-AWQ\"\n",
    "\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=hub_path, exist_ok=True)\n",
    "api.upload_folder(repo_id=hub_path, folder_path=quant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = load_dataset(\"BounharAbdelaziz/Arabic-Synthetic-Summarization-Dataset-Eval\", split=\"test\")\n",
    "metrics_df = load_dataset(\"BounharAbdelaziz/Arabic-Summarization-Eval-Metrics\", split=\"train\").to_pandas()\n",
    "avg_scores = load_dataset(\"BounharAbdelaziz/Arabic-Summarization-Eval-LLM-as-a-Judge\", split=\"train\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new column name\n",
    "metrics_df.columns = [['rouge1', 'rouge2', 'rougeL', 'rougeLsum', 'bleu', 'bertscore_precision', 'bertscore_recall', 'bertscore_f1', 'Model']]\n",
    "# change order\n",
    "metrics_df = metrics_df[['Model', 'rouge1', 'rouge2', 'rougeL', 'rougeLsum', 'bleu', 'bertscore_precision', 'bertscore_recall', 'bertscore_f1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten MultiIndex columns (if they exist)\n",
    "if isinstance(metrics_df.columns, pd.MultiIndex):\n",
    "    metrics_df.columns = metrics_df.columns.get_level_values(0)  # Keep only the first level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_path = \"BounharAbdelaziz/Qwen2.5-3B-Instruct-Summarizer-AWQ\"\n",
    "torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2228 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded_preds[0]: في صقلية قبل 1061 ميلاديًا، كانت الجزيرة مجزأة إلى خمس إمارات، وكان العرب والأشداء يتنافسون. بعد وفاة روجر الأول في 1101م، أصبحت باليرمو تحت حكم روجر الثاني. استمرت اللغة العربية في بلاط ملك النورمان حتى القرن 12. جورج الأنطاكي هو مثال على الشخصيات التي كانت تتحدث العربية في بلاطهم. أدرك فريدريك الثاني أهمية القصيدة العربية في بلاطه. بينما تظل صقلية جزيرة مسلمة في عهد الكلبيين، إلا أنها لم تفقد ثقافة العرب.\n",
      "decoded_refs[0]: في 1061، كانت صقلية مجزأة إلى خمس إمارات مع تنافس عربي وأمازيغي. استطاع الملك النورماني روeger الأول السيطرة عليها، وأصبحت باليرمو عاصمتها عام 1072. رغم فقدان العرب للسلطة السياسية، ظلوا الثقافة الرئيسية، وازدهرت فيها الأدب والعلم حتى القرن الثاني عشر. بعد وفاة روeger الأول، استمر هذا الوضع مع ابنه روجر الثاني.\n",
      "{'rouge1': 11.781275166703985, 'rouge2': 5.380885414174803, 'rougeL': 11.666032341378854, 'rougeLsum': 11.745504217336922, 'bleu': 2.677481236207293, 'bertscore_precision': (74.0169356736514,), 'bertscore_recall': (75.99423225667026,), 'bertscore_f1': 74.89298761964919}\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "IS_CAUSAL_LM = True\n",
    "MAX_LEN = 2048\n",
    "\n",
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(quantized_model_path, torch_dtype=torch_dtype).to(\"cuda\")\n",
    "model.use_cache = True\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_path)\n",
    "    \n",
    "# Set chat template\n",
    "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n",
    "    \n",
    "# set padding side deppending on model type\n",
    "tokenizer.padding_side= 'left'\n",
    "\n",
    "# Set reasonable default for models without max length\n",
    "tokenizer.model_max_length = MAX_LEN\n",
    "\n",
    "# Set pad_token_id equal to the eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "# get model name -> used in column name for saving summaries predictions in dataset\n",
    "model_name = quantized_model_path.split('/')[-1].strip()\n",
    "\n",
    "# get predictions and references\n",
    "predictions = eval_dataset[f\"summary_{quantized_model_path.split('/')[-1]}\"]\n",
    "references = eval_dataset[\"summary\"]\n",
    "\n",
    "# Convert to token IDs like during training\n",
    "pred_token_ids = [tokenizer.encode(p) for p in predictions]\n",
    "ref_token_ids = [tokenizer.encode(r) for r in references]\n",
    "\n",
    "# Use the exact same metric function from training\n",
    "eval_pred = (pred_token_ids, ref_token_ids)\n",
    "\n",
    "# compute metrics\n",
    "metrics = compute_metrics_causal_lm(eval_pred, tokenizer)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = eval_dataset.add_column(f\"summary_{quantized_model_path.split('/')[-1]}\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_col: summary_Qwen2.5-3B-Instruct-Summarizer-AWQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating summary_Qwen2.5-3B-Instruct-Summarizer-AWQ: 100%|██████████| 28/28 [03:14<00:00,  6.94s/it]\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# dataset that we will update\n",
    "updated_dataset = DatasetDict()\n",
    "\n",
    "model_names = []  # To track the model names\n",
    "\n",
    "# Prepare texts and summaries for batched processing\n",
    "texts = eval_dataset['text']\n",
    "\n",
    "# Process each model's summaries (you may iterate through all model summaries here)\n",
    "for model_col in eval_dataset.column_names:\n",
    "    if model_col == f\"summary_{quantized_model_path.split('/')[-1]}\":\n",
    "        print(f'model_col: {model_col}')\n",
    "        summaries = eval_dataset[model_col]\n",
    "        \n",
    "        model_name = model_col.replace(\"summary_\", \"\")\n",
    "        model_names.append(model_name)  # Store the model name for later\n",
    "        \n",
    "        # Process in batches with progress bar\n",
    "        updated_results = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=f\"Evaluating {model_col}\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_summaries = summaries[i:i + batch_size]\n",
    "            batch_scores = evaluate_quality_batch(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                batch_texts,\n",
    "                batch_summaries,\n",
    "                SYSTEM_PROMPT,\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "            updated_results.extend(batch_scores)\n",
    "        \n",
    "        # Save results in the dataset\n",
    "        eval_dataset = eval_dataset.add_column(f\"quality_score_{model_col}\", updated_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it only for the AWQ, we already have the others\n",
    "model_names = ['Qwen2.5-3B-Instruct-Summarizer-AWQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Model  LLM Average Score\n",
      "0  Qwen2.5-3B-Instruct-Summarizer-AWQ           1.536036\n"
     ]
    }
   ],
   "source": [
    "# Compute and display average scores for each model\n",
    "new_avg_scores = compute_average_scores(eval_dataset, model_names)\n",
    "print(new_avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The AWQ model scores pretty well in the LLM as a Judge evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>LLM Average Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen2.5-3B-Instruct-Summarizer-AWQ</td>\n",
       "      <td>1.536036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-3...</td>\n",
       "      <td>1.168919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0...</td>\n",
       "      <td>1.074324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1...</td>\n",
       "      <td>1.637387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1...</td>\n",
       "      <td>1.632883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gac...</td>\n",
       "      <td>1.403153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1...</td>\n",
       "      <td>1.398649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1...</td>\n",
       "      <td>1.443694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1...</td>\n",
       "      <td>1.436937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm...</td>\n",
       "      <td>0.520270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gn...</td>\n",
       "      <td>0.229730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gn...</td>\n",
       "      <td>0.189189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm...</td>\n",
       "      <td>0.036036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Model  LLM Average Score\n",
       "0                  Qwen2.5-3B-Instruct-Summarizer-AWQ           1.536036\n",
       "0   Qwen2.5-0.5B-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-3...           1.168919\n",
       "1   Qwen2.5-0.5B-Instruct-bs-2-lr-0.0001-ep-3-wp-0...           1.074324\n",
       "2   Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1...           1.637387\n",
       "3   Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1...           1.632883\n",
       "4   Falcon3-1B-Base-bs-1-lr-0.0001-ep-3-wp-0.1-gac...           1.403153\n",
       "5   Falcon3-1B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1...           1.398649\n",
       "6   Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1...           1.443694\n",
       "7   Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1...           1.436937\n",
       "8   mt5-base-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gnm...           0.520270\n",
       "9   mt5-small-bs-2-lr-0.001-ep-3-wp-0.1-gacc-16-gn...           0.229730\n",
       "10  mt5-small-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gn...           0.189189\n",
       "11  mt5-base-bs-2-lr-0.005-ep-3-wp-0.1-gacc-16-gnm...           0.036036"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_avg_scores_df = pd.DataFrame(new_avg_scores)\n",
    "all_avg_scores = pd.concat([new_avg_scores_df, avg_scores])\n",
    "all_avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1711.96ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "all_avg_scores_ds = Dataset.from_pandas(all_avg_scores).push_to_hub(\"BounharAbdelaziz/Arabic-Summarization-Eval-LLM-as-a-Judge\", commit_message=\"Added AWQ scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = eval_dataset.remove_columns(['quality_score_summary_Qwen2.5-3B-Instruct-Summarizer-AWQ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 22.85ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/BounharAbdelaziz/Arabic-Synthetic-Summarization-Dataset-Eval/commit/9149d8806f459bf32008ea27d59d1ec488147d8f', commit_message='Added AWQ summaries', commit_description='', oid='9149d8806f459bf32008ea27d59d1ec488147d8f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/BounharAbdelaziz/Arabic-Synthetic-Summarization-Dataset-Eval', endpoint='https://huggingface.co', repo_type='dataset', repo_id='BounharAbdelaziz/Arabic-Synthetic-Summarization-Dataset-Eval'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push new summaries\n",
    "eval_dataset.push_to_hub('BounharAbdelaziz/Arabic-Synthetic-Summarization-Dataset-Eval', commit_message=\"Added AWQ summaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell was run in another notebook, i just kept it here to see which columns (i.e. model) were used \n",
    "\n",
    "# select data for human eval\n",
    "selected_for_human_eval = [\n",
    "    'text', # we keep text to show it\n",
    "    'summary_Qwen2.5-3B-Instruct-bs-2-lr-0.0001-ep-3-wp-0.1-gacc-32-gnm-1.0-FP16-SFT-mx-2048-r-128-a-64-v5', \n",
    "    'summary_Qwen2.5-3B-Instruct-Summarizer-AWQ', \n",
    "    'summary_Falcon3-3B-Instruct-bs-1-lr-0.0001-ep-3-wp-0.1-gacc-1-gnm-1.0-FP16-SFT-mx-1024-r-256-a-128-v5'\n",
    "]\n",
    "\n",
    "human_eval_dataset = eval_dataset.select_columns(selected_for_human_eval)\n",
    "\n",
    "human_eval_dataset.push_to_hub('BounharAbdelaziz/Arabic-Summarization-Human-Eval-Summaries')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
